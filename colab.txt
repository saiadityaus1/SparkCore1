!wget -q https://github.com/saiadityaus1/test1/raw/main/df.csv -O df.csv
!wget -q https://github.com/saiadityaus1/test1/raw/main/df1.csv -O df1.csv
!wget -q https://github.com/saiadityaus1/test1/raw/main/dt.txt -O dt.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file1.txt -O file1.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file2.txt -O file2.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file3.txt -O file3.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file4.json -O file4.json
!wget -q https://github.com/saiadityaus1/test1/raw/main/file5.parquet -O file5.parquet
!wget -q https://github.com/saiadityaus1/test1/raw/main/file6 -O file6
!wget -q https://github.com/saiadityaus1/test1/raw/main/prod.csv -O prod.csv
!wget -q https://github.com/saiadityaus1/test1/raw/main/state.txt -O state.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/usdata.csv -O usdata.csv
!wget -q https://github.com/saiadityaus1/SparkCore1/raw/refs/heads/master/data.orc  -O data.orc
!wget -q https://raw.githubusercontent.com/saiadityaus1/SparkCore1/refs/heads/master/rm.json  -O rm.json
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz
!tar xf spark-3.1.2-bin-hadoop2.7.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"
import findspark
findspark.init()
import sys
import os
python_path = sys.executable
os.environ['PYSPARK_PYTHON'] = python_path
from pyspark import SparkConf
from pyspark import SparkContext
from pyspark.sql import SparkSession
conf = SparkConf().setAppName("pyspark").setMaster("local[*]").set("spark.driver.host","localhost").set("spark.driver.allowMultipleContexts","true")
sc = SparkContext(conf=conf)
spark = SparkSession.builder.config("spark.driver.allowMultipleContexts","true").getOrCreate()
print("DONE==== START YOUR WORKðŸ‘‡ ")
